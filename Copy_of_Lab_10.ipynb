{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidSchineis/Math-Physics/blob/main/Copy_of_Lab_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Abstract\n",
        "This lab introduces neural networks using TensorFlow and Keras. We began by training a linear model to perform regression, learning how the learning rate effects results. We then extended the model with more layers and different activation functions to better fit y=ln(x+1). Then, we explored overfitting by comparing training and validation losses over epochs, identifying where validation loss stopped improving. Finally, we applied all of this to the MINST handwritten digits dataset. We trained a neural network to recognize digits and evaluated its performance with a confusion matrix. This lab demonstrates how neural networks can be built, trained, and evaluated on real problems in Python."
      ],
      "metadata": {
        "id": "MZMkr_XozA3F"
      },
      "id": "MZMkr_XozA3F"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4ynW2r5_1n1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns"
      ],
      "id": "F4ynW2r5_1n1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXEZ1Ich_1n2"
      },
      "source": [
        "Today, we will provide a brief introduction to neural networks. Neural networks are designed to take a set of inputs and learn a model composed of functions that can process these inputs to generate a specific output. This concept is similar to matrix operations, where you can consider the inputs and outputs as known matrices. During the training process, the goal is to find a matrix that can be multiplied with the inputs to transform them and produce the desired output.\n",
        "\n",
        "We will begin with an example of using neural net to perform a linear regression. We will first generate a random noisy dataset.\n",
        "\n",
        "Note, as you are writing this code, do not use \"Run All\" button - if you overwrite some variables, it may break some of the logic."
      ],
      "id": "fXEZ1Ich_1n2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e18OzWYi_1n3"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "x = np.arange(0,10,0.1)\n",
        "y = 2*x+3+np.random.normal(size=len(x))\n",
        "\n",
        "plt.scatter(x,y,label='random data')\n",
        "\n",
        "plt.plot(x,2*x+3,c='red',label='underlying relation')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "id": "e18OzWYi_1n3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmavhqNp_1n3"
      },
      "source": [
        "We will now construct a simple model consisting of a single input (x), and a single output (y)\n"
      ],
      "id": "rmavhqNp_1n3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_oik9Tb_1n3"
      },
      "outputs": [],
      "source": [
        "# Clear any existing TensorFlow graph\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "# Define the input layer\n",
        "model.add(tf.keras.Input(shape=(1,)))\n",
        "# Define the fully connected dense layer that would consist of our outputs\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "print(model.summary())"
      ],
      "id": "Y_oik9Tb_1n3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MOgtRjR_1n3"
      },
      "source": [
        "Before using it, the model needs to be compiled, specifying what the neural net should pay attention to. The way it is doing it is through evaluating the loss. In the process of training, the goal is to minimize the loss. What the loss is can be defined in a number of different ways, but for regression tasks, it is common to use mean squred error, i.e.\n",
        "$$MSE=\\frac{1}{n}\\sum_i^n(y-p)^2$$\n",
        "where n is the number of points, y are the outputs, and p are the predictions.\n",
        "\n",
        "Additionally, the model needs an optimizer, which is an algorithm to adjust the parameters of a model during the training process. Adam or SDG are two popular choices.\n",
        "\n",
        "Both of these optimizers depend on a particular learning rate, which roughly controls the relative scale by which the weights of the model get adjusted. If the learning rate is too high, it will perform very large modifications to the weights, and the model may fail to converge. If the learning rate is too low, it may be unnecessarily slow to converge.\n",
        "\n",
        "Afterwards, we are ready to train the model. During the training process, the model will make predictions, evaluate the loss, adjust the weights, and repeat these steps for a fixed number of epochs. Each epoch can be broken into several batches that are each adjusted separately."
      ],
      "id": "2MOgtRjR_1n3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ts38dro_1n4"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=100.))\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x, y, epochs=100, verbose=False,batch_size=100)\n",
        "\n",
        "# Make the predictions\n",
        "p=model.predict(x)\n",
        "\n",
        "\n",
        "\n",
        "plt.scatter(x,y,label='random data')\n",
        "plt.plot(x,p,c='orange',label='predictions')\n",
        "plt.plot(x,2*x+3,c='red',label='underlying relation')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'],label='loss over training epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "3Ts38dro_1n4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0TOJiAk_1n4"
      },
      "source": [
        "Every single training run is random, producing a different set of predictions. However, for a given set of hyperparameters, the training is likely to take a predictable path.\n",
        "\n",
        "Most likely, the resulting fit was roughly representative of the data, but it was far from ideal.\n",
        "\n",
        "Train the 8 different models, adjusting the learning rate from $10^{-5}$ to $10^2$, evenly logarithmically spaced. After every training session, plot loss over all of the epochs, keeping all of the outputs on the same graph. Set the y scale to be from 0.5 to 3.\n",
        "\n",
        "Remember that the goal is to minimize the loss, and to do it as fast as possible. What is the best learning rate in order to accomplish this ?\n",
        "\n",
        "#### Answer\n",
        "The best learning rate to accomplish this is $10^{-1}$. See the graph below for justification of minimizing loss."
      ],
      "id": "A0TOJiAk_1n4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PX1T8-B-_1n4"
      },
      "outputs": [],
      "source": [
        "learnRate = np.logspace(-5, 2, 8)\n",
        "final_losses = []\n",
        "\n",
        "plt.figure()\n",
        "for lr in learnRate:\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = tf.keras.Sequential()\n",
        "    # Define the input layer\n",
        "    model.add(tf.keras.Input(shape=(1,)))\n",
        "    # Define the fully connected dense layer that would consist of our outputs\n",
        "    model.add(tf.keras.layers.Dense(1))\n",
        "    # Compile the model\n",
        "    model.compile(loss='mean_squared_error',\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=lr))\n",
        "    # Train the model\n",
        "    history = model.fit(x, y, epochs=100, verbose=False,batch_size=100)\n",
        "    # Make the predictions\n",
        "    p=model.predict(x)\n",
        "\n",
        "    plt.plot(history.history['loss'], label=f'lr={lr:g}')\n",
        "    print(lr)\n",
        "\n",
        "\n",
        "plt.ylim(0.5, 3)\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "id": "PX1T8-B-_1n4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Caption\n",
        "This is a plot of the loss vs epoch for varying learning rates of a sequential model. The learning rates vary from $10^{-5}$ to $10^{2}$."
      ],
      "metadata": {
        "id": "TTC0TBMrdgDD"
      },
      "id": "TTC0TBMrdgDD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-97t2Q5P_1n5"
      },
      "source": [
        "Show the predictions of a model trained with this learning rate."
      ],
      "id": "-97t2Q5P_1n5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B32WZSqG_1n5"
      },
      "outputs": [],
      "source": [
        "\n",
        "tf.keras.backend.clear_session()\n",
        "model = tf.keras.Sequential()\n",
        "# Define the input layer\n",
        "model.add(tf.keras.Input(shape=(1,)))\n",
        "# Define the fully connected dense layer that would consist of our outputs\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "# Compile the model\n",
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.1))\n",
        "# Train the model\n",
        "history = model.fit(x, y, epochs=100, verbose=False,batch_size=100)\n",
        "\n",
        "# Make the predictions\n",
        "p=model.predict(x)\n",
        "\n",
        "\n",
        "\n",
        "plt.scatter(x,y,label='random data')\n",
        "plt.plot(x,p,c='orange',label='predictions')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "id": "B32WZSqG_1n5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Caption\n",
        "This plots the prediction of our sequential model at a learning rate of $10^{-1}$ against our data set of points where y = 2x+3"
      ],
      "metadata": {
        "id": "0BEx_YUFfRd-"
      },
      "id": "0BEx_YUFfRd-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMqSMB8k_1n5"
      },
      "source": [
        "Let's try a different dataset. Train model with the same architecture that is based on the function $y=\\ln(x+1)$"
      ],
      "id": "nMqSMB8k_1n5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1Cxw0Jj_1n5"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "x = np.arange(0,10,0.1)\n",
        "y = np.log(x+1)+np.random.normal(size=len(x))*0.1\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.Input(shape=(1,)))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "# Compile the model\n",
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.1))\n",
        "# Train the model\n",
        "history = model.fit(x, y, epochs=100, verbose=False,batch_size=100)\n",
        "\n",
        "# Make the predictions\n",
        "p=model.predict(x)\n",
        "\n",
        "\n",
        "\n",
        "plt.scatter(x,y,label='random data')\n",
        "plt.plot(x,p,c='orange',label='predictions')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "id": "s1Cxw0Jj_1n5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Caption\n",
        "This plots the prediction of our sequential model at a learning rate of $10^{-1}$ against our data set of points where y = ln(x+1)."
      ],
      "metadata": {
        "id": "AY_vkzu-hdhF"
      },
      "id": "AY_vkzu-hdhF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJRdPrwI_1n6"
      },
      "source": [
        "The current model is insufficient to handle the complexity of the function. To enhance its performance, we can incorporate additional layers. Instead of directly producing a single output from a single input, we will introduce hidden layers in between. These layers will consist of 3 neurons (divideing the data into 3 distinct channels), each conditioned uniquely, which are subsequently merged to generate the prediction. This approach allows for a more comprehensive and nuanced representation of the data, potentially improving the model's capabilities.\n",
        "\n",
        "Use this model to produce a more faithful prediction. Experiment with different parameters, including the learning rate and the batch size in order to achieve this. Make sure to run it a couple of times to ensure a self-consistent performance."
      ],
      "id": "uJRdPrwI_1n6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GM8hT69-_1n6"
      },
      "outputs": [],
      "source": [
        "x = np.arange(0,10,0.1)\n",
        "y = np.log(x+1)+np.random.normal(size=len(x))*0.1\n",
        "\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.Input(shape=(1,)))\n",
        "model.add(tf.keras.layers.Dense(3,activation='tanh'))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.1))\n",
        "# Train the model\n",
        "history = model.fit(x, y, epochs=100, verbose=False,batch_size=200)\n",
        "\n",
        "# Make the predictions\n",
        "p=model.predict(x)\n",
        "\n",
        "plt.scatter(x,y,label='random data')\n",
        "plt.plot(x,p,c='orange',label='predictions')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "GM8hT69-_1n6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Caption\n",
        "This plots the prediction of our sequential model, with an added dense layer of with tanh activation function, at a learning rate of $10^{-1}$ against our data set of points where y = ln(x+1)."
      ],
      "metadata": {
        "id": "0SfcZwtYkZGO"
      },
      "id": "0SfcZwtYkZGO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0iGwVTL_1n6"
      },
      "source": [
        "In the model above, we introduced an activation function for the hidden layers.\n",
        "Activation functions are mathematical functions applied to the output of a neuron in a neural network. They introduce non-linearity to the network, enabling it to learn and model complex relationships in the data.\n",
        "\n",
        "Here are explanations of some commonly used activation functions:\n",
        "\n",
        "1. sigmoid: The sigmoid function, also known as the logistic function, maps the input to a value between 0 and 1. It is often used in binary classification problems where the output represents the probability of belonging to a certain class. However, it can suffer from vanishing gradients and is not commonly used in deeper networks.\n",
        "\n",
        "2. relu (Rectified Linear Unit): The ReLU function sets all negative values to zero and keeps positive values unchanged. It is the most widely used activation function due to its simplicity and ability to mitigate the vanishing gradient problem. ReLU works well in most cases but can cause dead neurons (i.e., neurons that output zero) during training.\n",
        "\n",
        "3. tanh (Hyperbolic Tangent): The tanh function maps the input to a value between -1 and 1. It is symmetric around the origin and is useful in models where negative values are meaningful. Tanh can be used in both hidden layers and output layers.\n",
        "\n",
        "4. softmax: The softmax function is commonly used in multi-class classification problems. It converts a vector of real numbers into a probability distribution, where the sum of all probabilities is equal to 1. Softmax is useful when dealing with mutually exclusive classes.\n",
        "\n",
        "5. linear: The linear activation function simply outputs the input value without any transformation. It is primarily used in regression problems where the output can be any real value.\n",
        "\n",
        "The choice of activation function depends on the problem at hand and the characteristics of the data. Experimentation and understanding the behavior of different activation functions can help in selecting the most suitable one for a particular neural network architecture."
      ],
      "id": "Y0iGwVTL_1n6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwIb9BcT_1n6"
      },
      "source": [
        "When dealing with more complex models, you should be careful to prevent overfitting. You should have not only a substantively large training set, but also a development and/or test sets with which the model isn't familiar, that you can use to vet your predictions. Validation set is used during training, evaluating it after every epoch, without affecting any of the weights. Test set is used after training.\n",
        "\n",
        "----\n",
        "\n",
        "Run the following code (it will take a little while longer to compute due to a larger number of epochs, especially running it on CPU instead of GPU, so please be patient). Then compare the difference in loss between the training and validation sets."
      ],
      "id": "qwIb9BcT_1n6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6M3ZW7K5_1n6"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "x = np.arange(0,10)\n",
        "y = np.log(x+1)+np.random.normal(size=len(x))*0.2\n",
        "x1 = np.arange(0,10,0.1)\n",
        "y1 = np.log(x1+1)+np.random.normal(size=len(x1))*0.2\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.Input(shape=(1,)))\n",
        "model.add(tf.keras.layers.Dense(3,activation='tanh'))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=1e-2))\n",
        "history = model.fit(x, y, epochs=500, verbose=False,batch_size=10,validation_data=(x1,y1))\n",
        "p=model.predict(x)\n",
        "\n",
        "\n",
        "plt.scatter(x,y,label='random data')\n",
        "plt.scatter(x1,y1,label='validation data')\n",
        "plt.plot(x,p,c='blue',label='predictions')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(history.history['loss'],label='loss over training epochs')\n",
        "plt.plot(history.history['val_loss'],label='validation loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.yscale('log')\n",
        "plt.xscale('log')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "6M3ZW7K5_1n6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vUXhcXk_1n6"
      },
      "source": [
        "You might notice that the loss in the training set is eventually reaching the point where it becomes quite small, since the model was able to \"memorize\" the position of these 10 points. The loss in dev set, on the other hand, initially tracks the loss in the training set before starting to lag behind significantly. There are a number of best practices one could employ to prevent it from happening, including terminating the training at a point when dev loss stops improving.\n",
        "\n",
        "#### Question\n",
        "\n",
        "At what epoch does the loss in the validation set stops substantively improving?"
      ],
      "id": "-vUXhcXk_1n6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Answer\n",
        "The validation loss stops substantively improving right around the 10^2 epoch."
      ],
      "metadata": {
        "id": "EhyYjM7AAoFQ"
      },
      "id": "EhyYjM7AAoFQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0me75vkB_1n7"
      },
      "source": [
        "----\n",
        "In addition to performing regression tasks, it is possible to create labels for classification. For this we will load MNIST dataset of handwritten digits"
      ],
      "id": "0me75vkB_1n7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNNXLg4__1n7"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()"
      ],
      "id": "NNNXLg4__1n7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BC8M3Qhk_1n7"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(test_images[i], cmap='gray')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_title(f\"Label: {test_labels[i]}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "BC8M3Qhk_1n7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGrFVovl_1n7"
      },
      "source": [
        "For classification, rather than a mean sqared error loss, a popular choice of loss is sparse categorical crossentropy. And the goal of the model can change from minimizing the loss to maximizing the accuracy (these goals are not fully equivalent, but they do correlate with each other strongly).\n",
        "\n",
        "We will redefine our model architecture to take a 2d image, and it will produce 10 outputs, each one representing a probability of an image corresponding to a particular digit."
      ],
      "id": "rGrFVovl_1n7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNMEj-k8_1n7"
      },
      "outputs": [],
      "source": [
        "# Preprocess the data - it is usually suggested to normalize the input data\n",
        "# in such a way that it would fit in the range of -1 to 1 or 0 to 1\n",
        "train_images_norm = train_images / 255.0\n",
        "test_images_norm = test_images / 255.0\n",
        "\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.Input(shape=(28,28)))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(128,activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(10,activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_images_norm, train_labels, epochs=10)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images_norm, test_labels, verbose=2)\n",
        "print('Test accuracy:', test_acc)\n",
        "\n",
        "predictions = tf.argmax(model.predict(test_images),axis=1).numpy()\n"
      ],
      "id": "uNMEj-k8_1n7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcKhKJNJ_1n7"
      },
      "outputs": [],
      "source": [
        "#confirming that the predictions are accurate\n",
        "fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(test_images[i], cmap='gray')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_title(f\"Predicted: {predictions[i]}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "kcKhKJNJ_1n7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell below willl create a confusion matrix, i.e., showing the frequency of cases where the predictions were accurate, vs where they were missmatched, for each of the classes"
      ],
      "metadata": {
        "id": "zKzfzvEbA5u6"
      },
      "id": "zKzfzvEbA5u6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Re5_0t6_1n8"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(test_labels, predictions)\n",
        "accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "\n",
        "# Create a heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=False, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "\n",
        "# Set labels, title, and accuracy\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.title(f\"Confusion Matrix (Accuracy: {accuracy:.2f})\")\n",
        "\n",
        "# Show the heatmap\n",
        "plt.show()"
      ],
      "id": "9Re5_0t6_1n8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tflk9Bby_1n8"
      },
      "source": [
        "Use np.where function to find the images where there is a mismatch between test labels and the predictions.\n",
        "Display 9 such cases, similarly to the above, and show what was the predicted class for these images."
      ],
      "id": "tflk9Bby_1n8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARH_gLqp_1n8"
      },
      "outputs": [],
      "source": [
        "x=np.random.permutation(np.arange(len(predictions)))\n",
        "predictions=predictions[x]\n",
        "test_images=test_images[x]\n",
        "test_labels=test_labels[x]\n",
        "\n",
        "a=np.where(predictions != test_labels)[0]\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(test_images[a[i]], cmap='gray')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_title(f\"True: {test_labels[a[i]]}, Pred: {predictions[a[i]]}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "ARH_gLqp_1n8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Caption\n",
        "9 examples of misclassified digits showing true and predicted labels for each."
      ],
      "metadata": {
        "id": "oIWP-_T55oQ0"
      },
      "id": "oIWP-_T55oQ0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4Mdcu2J_1n9"
      },
      "source": [
        "### Extra credit\n",
        "\n",
        "Build a model and make predictions for dataset presented below. Tune the hyperparameters (including the number of hidden layers, number of neurons, learning rate, and batch size) to produce a good fit; keep the number of epochs to no more than 100, for the sake of speed."
      ],
      "id": "A4Mdcu2J_1n9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnfqoZd__1n9"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "x = np.linspace(0,np.pi*5,1000)\n",
        "y = np.sin(x)+np.random.normal(size=len(x))*0.1\n",
        "x1 = np.random.random(100)*np.pi*5\n",
        "y1 = np.sin(x1)+np.random.normal(size=len(x1))*0.1\n",
        "\n",
        "# put your code here"
      ],
      "id": "EnfqoZd__1n9"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "generative_ai_disabled": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}